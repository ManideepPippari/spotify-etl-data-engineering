ğŸµ Spotify End-to-End Data Engineering Pipeline (Production-Grade)


AWS S3 | Lambda | EventBridge | Glue | Athena | Docker Airflow | Snowflake | Databricks (PySpark) | Power BI

This project demonstrates a full, enterprise-level ETL + ELT pipeline, ingesting Spotify artist track data, transforming it across multi-cloud systems, and loading it into analytics and BI layers.

Built using 11 industry technologies, this system simulates what a real Data Engineering team builds for scalable analytics.



ğŸ§  High-Level Architecture

![Architecture Diagram](architecture/spotify_etl_architecture.png)

Full Workflow
	1.	Spotify API â†’ Lambda (Ingestion)
	2.	Lambda â†’ S3 (Raw/Processed Data)
	3.	Glue Crawler â†’ Athena (Schema inference + querying)
	4.	Airflow (Docker) â†’ Orchestrates Athena, Glue, Databricks, Snowflake jobs
	5.	Databricks (PySpark) â†’ Bronze â†’ Silver â†’ Gold tables
	6.	Snowflake â†’ ELT transformations + stored procedures + tasks
	7.	Power BI â†’ Final visualization layer



    ğŸ›  Tech Stack

Cloud: AWS S3, Lambda, EventBridge, Glue, Athena
Orchestration: Apache Airflow (Docker)
Processing: Databricks, PySpark
Data Warehouse: Snowflake
Analytics: Power BI
Programming: Python 3
CI/CD: Git + GitHub


ğŸ“‚ Repository Structure


spotify-etl-data-engineering/
â”‚
â”œâ”€â”€ src/                       # Local Spotify ingestion + transform scripts
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ transform/
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ lambda/                    # Lambda ingestion + transform code + configs
â”‚
â”œâ”€â”€ Eventbridge/               # EventBridge schedule + setup docs
â”‚
â”œâ”€â”€ glue/
â”‚   â”œâ”€â”€ crawler_config.json
â”‚   â”œâ”€â”€ processed_schema.json
â”‚   â”œâ”€â”€ transformed_schema.json
â”‚   â””â”€â”€ s3_samples/
â”‚
â”œâ”€â”€ athena/
â”‚   â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ workgroup_config.json
â”‚   â”œâ”€â”€ results_samples/
â”‚   â””â”€â”€ top10_artist_stats_query.sql
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ tables/
â”‚   â””â”€â”€ result_samples/
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ procedures/
â”‚   â”œâ”€â”€ tables/
â”‚   â””â”€â”€ results_samples/
â”‚
â”œâ”€â”€ dags/                      # Airflow DAG + CSV sample
â”‚
â”œâ”€â”€ s3/                        # Local copies of S3 objects
â”‚
â””â”€â”€ powerbi/                  



ğŸ§ Spotify Ingestion

Python | Spotify API

Features
	â€¢	Fetches Top Tracks from multiple artists
	â€¢	Gets Access Token automatically
	â€¢	Saves tracks_raw.csv and tracks_processed.csv
	â€¢	Environment-variable driven (no secrets in repo)

AWS Lambda Ingest Function
	â€¢	Calls Spotify API
	â€¢	Writes processed tracks to S3 â†’ spotify/processed/

EventBridge Integration
	â€¢	Runs Lambda every hour
	â€¢	JSON config included:
	â€¢	eventbridge_schedule.json
	â€¢	Full setup instructions


â˜ AWS Layer

spotify/
    raw/
    processed/
    transformed/
    athena-results/

    ğŸŸ£  AWS Glue & Athena (Data Discovery + SQL Analytics)

Glue Crawler
	â€¢	Auto-creates schema for processed and transformed tables
	â€¢	All configurations stored:
	â€¢	crawler_config.json
	â€¢	processed_schema.json
	â€¢	transformed_schema.json

Athena
	â€¢	Workgroup: spotify-analytics-wg
	â€¢	Output path: /spotify/athena-results/
	â€¢	Includes:
	â€¢	DDL for both tables
	â€¢	Artist analytics SQL
	â€¢	Top 10 artist stats result CSV


âš™ Orchestration
    ğŸŸ¢ 4. Airflow (Docker) Orchestration

Airflow DAG does:
	1.	Run local Spotify extract â†’ CSV
	2.	Upload CSV to S3
	3.	Trigger Glue Crawler
	4.	Run Athena Query
	5.	Trigger Databricks workflow
	6.	Run Snowflake procedures
	7.	Run Data Quality Checks
	8.	Final summary output

Folder: dags/spotify_etl_dag.py
Includes:
	â€¢	XCom usage
	â€¢	SnowflakeOperator
	â€¢	HTTP Hook for Databricks
	â€¢	Retry & error handling
	â€¢	Full documentation


ğŸ”¥ Databricks
PySpark | Delta-Lake

Notebook:

âœ” Spotify_Spark_Processing.ipynb
âœ” .py version
âœ” HTML export version


ğŸ¥‰ BRONZE â€” Raw ingest
	â€¢	Created table: spotify_tracks_bronze
	â€¢	COPY INTO from S3 processed

ğŸ¥ˆ SILVER â€” Cleaned & typed
	â€¢	Deduping
	â€¢	Type casting
	â€¢	Calculated fields
	â€¢	Table: spotify_tracks_silver

ğŸ¥‡ GOLD â€” Analytics-ready
	â€¢	Table: spotify_tracks_gold
	â€¢	Aggregates
	â€¢	Feature engineering
	â€¢	Joins
	â€¢	Popularity summary table

Databricks job config also included:
	â€¢	cluster config markdown
	â€¢	job markdown
	â€¢	SQL queries
	â€¢	Result samples


â„ Snowflake
Warehouse | ELT | Tasks

âœ” Database
âœ” Schema
âœ” Warehouse
âœ” File formats
âœ” Stages
âœ” Tables (Bronze, Silver, Gold, Raw, History)
âœ” Views
âœ” Stored Procedures
âœ” Tasks (scheduled pipelines)
âœ” DQ checks
âœ” Result samples


ğŸ“Š Power BI
Visualization

/powerbi/
    spotify_dashboard.pbix
    screenshots/

	â€¢	Top Artists by Popularity
	â€¢	Track Duration Distribution
	â€¢	Explicit vs Non-Explicit Analysis
	â€¢	Bronze â†’ Silver â†’ Gold Row Counts
	â€¢	Release Timeline


 ğŸ§ª 8. Data Quality Checks (Across Platforms)

You included:
	â€¢	Athena row count
	â€¢	Snowflake row count
	â€¢	Cross-system DQ check
	â€¢	Gold table summary
	â€¢	Popularity stats analytics


ğŸ¬ 9. Demo Video

/demo/
   pipeline_demo.mp4



ğŸ§© 10. How to Run This Project (Locally / Cloud)

Local (Airflow + Docker)
docker-compose up -d

Upload DAG:
dags/spotify_etl_dag.py

Check UI:
localhost: